{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7dbfa9",
   "metadata": {},
   "source": [
    "# Report for Data Wrangling\n",
    "## Gathering Stage\n",
    "This dataset was very much complicated because it needed the gathering of data from various sources, using various methods. First I had to create a Twitter developer account, for being able to access Twitter data for the dataset. The Twitter data would include The tweets, the tweet id, the picture on the tweet, the source where the tweet was twitted from and the time it occurred. The second dataset contained images as well as their URL and the number of images connected to the specific tweet. The images were predicted in the likelihood of them matching the dog in the specified tweet. There were three predictions with each having a bool value(True/false), a name-value (golden retriever) and a confidence percentage value(ie how confident the prediction is that the dog in the picture is the actual dog mentioned in the tweet. The third dataset contained the retweet count as well as the favourite count. The dataset needs to be read as a JSON file from a URL and required using the request library and IO library.\n",
    "In this stage, I also made use of the tweepy library to access the Twitter data and I had to first confirm and get authorisation by inserting the access key that I received from the Twitter developer account.\n",
    "## Assessing Stage\n",
    "The assessing stage was quite demanding since there are three datasets. The framework for assessing I used was to implement two types of assessments on the given datasets. The first is Visual Assessment and the second is the Programmatic assessment. In this stage of Data wrangling, we were focused on assessing the data based on two categories. Quality of data and Tidiness or structural issues. The first dataset had 90% of the issues and also had the most records and columns. The second dataset and the third dataset did not have any null values. The first dataset had a lot of issues with some of them being unnecessary columns, consistency in naming conventions, a lot of data types that needed to be changed to their correct format, conversion of columns to rows and removal of retweet values, and some columns had 60% of their data missing which rendered them useless for analysis e.t.c. I made use of methods such as value_count() , sample() , head() ,duplicated() among others.\n",
    "## Cleaning Stage\n",
    "**NB Before cleaning was done** I made copies of the original dataset\n",
    "Cleaning was rather challenging but also a lot of fun and a lot of research in trying to solve problems related to the datasets. Pandas have a lot of methods that help with cleaning and I made sure to make use of those methods when cleaning the datasets. The framework I used for cleaning the dataset is Define, Code, and Test. The define stage lets me explain the problem, the Code stage is where I attempt to solve the problem, and the Test stage is where I check to see if the problem has been resolved. Some problems were easier to solve than others. Some methods i used include , replace() ,apply() ,len() , astype() and a new and exciting method I learnt called mapping or map(). The rating column was based on two columns ie rating_numerator and rating_denominator. These were integer numbers but I needed the number as a fraction and not necessarily as decimals. So to avoid the numbers being decimals I converted both columns to string and concatenated them in such a manner that they appear as fractions since this was vital for analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
